"""Generate individual posts from news articles for @ai_dlya_doma channel."""

import json
import os
import re
from dataclasses import dataclass
from enum import Enum
from typing import Dict, List, Optional, Tuple

from anthropic import (
    Anthropic,
    APIConnectionError,
    APITimeoutError,
    RateLimitError,
)
from openai import OpenAI
from tenacity import (
    retry,
    retry_if_exception_type,
    stop_after_attempt,
    wait_exponential,
)

from logger import get_logger

logger = get_logger("news_bot.post_generator")


def parse_classifier_response(response_text: str) -> dict:
    """
    Parse classifier response with error handling.
    Returns default response if LLM returned invalid JSON.
    """
    DEFAULT_RESPONSE = {
        "relevant": False,
        "confidence": 0,
        "category": "parse_error",
        "audience": "unknown",
        "format": "ai_tool",
        "reason": "Failed to parse LLM response",
        "needs_review": True,
        "url_check_needed": True,
    }

    try:
        # Remove markdown code blocks if present
        cleaned = re.sub(r"^```json\s*", "", response_text.strip())
        cleaned = re.sub(r"\s*```$", "", cleaned)

        # Try to find JSON in text
        json_match = re.search(r"\{[^{}]*\}", cleaned, re.DOTALL)
        if json_match:
            cleaned = json_match.group()

        data = json.loads(cleaned)

        # Validate required fields
        if "relevant" not in data or "confidence" not in data:
            data = DEFAULT_RESPONSE.copy()
            data["reason"] = "Missing required fields"
            return data

        # Normalize confidence to 0-100
        data["confidence"] = max(0, min(100, int(data.get("confidence", 0))))

        # Defaults for optional fields
        data.setdefault("category", "unknown")
        data.setdefault("audience", "consumer")
        data.setdefault("format", "ai_tool")
        data.setdefault("reason", "")
        data.setdefault("needs_review", data["confidence"] < 70)
        data.setdefault("url_check_needed", False)

        # Filter out enterprise content
        audience = data.get("audience", "consumer").lower()
        if audience == "enterprise":
            data["relevant"] = False
            data["reason"] = f"Enterprise content filtered: {data.get('reason', '')}"
            logger.info(f"Filtered enterprise content: {data.get('reason', '')[:50]}")
        elif audience == "mixed":
            # Lower confidence for mixed audience
            data["confidence"] = max(0, data["confidence"] - 10)
            data["needs_review"] = True

        return data

    except (json.JSONDecodeError, TypeError, ValueError) as e:
        response = DEFAULT_RESPONSE.copy()
        response["reason"] = f"JSON parse error: {str(e)[:50]}"
        return response


def validate_telegram_html(text: str) -> str:
    """
    Validate and fix common HTML issues for Telegram.

    Telegram supports: <b>, <i>, <u>, <s>, <code>, <pre>, <a href="">
    """
    if not text:
        return text

    # Allowed Telegram HTML tags
    allowed_tags = ['b', 'i', 'u', 's', 'code', 'pre', 'a']

    # Count open and close tags
    for tag in allowed_tags:
        open_count = len(re.findall(rf'<{tag}[^>]*>', text, re.IGNORECASE))
        close_count = len(re.findall(rf'</{tag}>', text, re.IGNORECASE))

        # If imbalanced, try to fix or remove
        if open_count != close_count:
            logger.warning(f"HTML tag <{tag}> imbalanced: {open_count} open, {close_count} close")
            # Remove all instances of this tag if imbalanced
            text = re.sub(rf'<{tag}[^>]*>', '', text, flags=re.IGNORECASE)
            text = re.sub(rf'</{tag}>', '', text, flags=re.IGNORECASE)

    # Fix common LLM mistakes with <a> tags
    # Fix: <a href = "url"> ‚Üí <a href="url">
    text = re.sub(r'<a\s+href\s*=\s*["\']([^"\']+)["\']>', r'<a href="\1">', text)

    # Fix: missing quotes around href
    text = re.sub(r'<a\s+href=([^"\'\s>]+)>', r'<a href="\1">', text)

    # Remove any unsupported HTML tags
    text = re.sub(r'<(?!/?(?:b|i|u|s|code|pre|a)[^>]*>)[^>]+>', '', text)

    return text.strip()


def is_good_image(url: str) -> bool:
    """
    Check if OG-image URL is likely a good quality image.
    Returns False for placeholders, logos, icons, etc.
    """
    if not url:
        return False

    url_lower = url.lower()

    # Bad patterns - likely placeholders or low-quality images
    bad_patterns = [
        'placeholder', 'default', 'logo', 'icon', 'avatar',
        '1x1', '1x1.', 'pixel', 'blank', 'empty', 'spacer',
        'og-default', 'social-default', 'share-default',
        'no-image', 'noimage', 'missing'
    ]

    if any(pattern in url_lower for pattern in bad_patterns):
        return False

    # Check for valid image extension
    valid_extensions = ['.jpg', '.jpeg', '.png', '.webp', '.gif']
    has_valid_ext = any(url_lower.endswith(ext) or f'{ext}?' in url_lower
                        for ext in valid_extensions)

    # Some URLs don't have extensions but are still valid (e.g., CDN URLs)
    # Accept them if they don't match bad patterns
    if not has_valid_ext:
        # Check for common image CDN patterns
        cdn_patterns = ['cloudinary', 'imgix', 'cloudfront', 'akamai',
                        'fastly', 'cdn', 'images', 'media', 'assets']
        if not any(cdn in url_lower for cdn in cdn_patterns):
            return False

    return True


def generate_image_via_openai(prompt: str) -> Optional[str]:
    """
    Generate image using OpenAI DALL-E 3.
    Returns URL of generated image or None on error.
    """
    api_key = os.getenv('OPENAI_API_KEY')
    if not api_key:
        logger.warning("OPENAI_API_KEY not set, cannot generate image")
        return None

    try:
        client = OpenAI(api_key=api_key)
        response = client.images.generate(
            model="dall-e-3",
            prompt=prompt,
            size="1024x1024",
            quality="standard",
            n=1
        )
        return response.data[0].url
    except Exception as e:
        logger.error(f"OpenAI image generation failed: {e}")
        return None


def get_image_for_post(article: Dict, image_prompt: str = None) -> Tuple[Optional[str], str]:
    """
    Get image for post using hybrid approach:
    1. Try OG-image from article if it's good quality
    2. Fall back to OpenAI generation

    Returns:
        Tuple of (image_url, source_type)
        source_type: 'og_image', 'generated', or 'none'
    """
    # Try OG-image first
    og_image = article.get('image_url')
    if og_image and is_good_image(og_image):
        logger.info(f"Using OG-image: {og_image[:50]}...")
        return (og_image, 'og_image')

    # Generate if we have a prompt
    if image_prompt:
        logger.info("OG-image not suitable, generating via OpenAI...")
        generated_url = generate_image_via_openai(image_prompt)
        if generated_url:
            return (generated_url, 'generated')

    logger.warning("No image available for post")
    return (None, 'none')


class PostFormat(Enum):
    """Types of posts for the channel."""
    AI_TOOL = "ai_tool"          # AI-–Ω–∞—Ö–æ–¥–∫–∞ –¥–Ω—è
    QUICK_TIP = "quick_tip"      # –ë—ã—Å—Ç—Ä—ã–π —Å–æ–≤–µ—Ç
    PROMPT_DAY = "prompt_day"    # –ü—Ä–æ–º—Ç –¥–Ω—è
    COMPARISON = "comparison"    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ
    CHECKLIST = "checklist"      # –ß–µ–∫-–ª–∏—Å—Ç


@dataclass
class GeneratedPost:
    """A generated post ready for publication."""
    text: str
    format: PostFormat
    article_url: str
    article_title: str
    image_prompt: Optional[str] = None
    image_url: Optional[str] = None  # OG/RSS image URL from article


class PostGenerator:
    """Generate beautiful posts for Telegram channel."""

    def __init__(self, api_key: str = None):
        """Initialize with Anthropic API."""
        self.api_key = api_key or os.getenv("ANTHROPIC_API_KEY")
        if not self.api_key:
            raise ValueError("ANTHROPIC_API_KEY not found")

        self.client = Anthropic(api_key=self.api_key)
        self.haiku_model = "claude-3-haiku-20240307"
        self.sonnet_model = "claude-sonnet-4-20250514"

    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=2, min=4, max=60),
        retry=retry_if_exception_type(
            (RateLimitError, APIConnectionError, APITimeoutError)
        ),
        before_sleep=lambda retry_state: logger.warning(
            f"Claude API retry {retry_state.attempt_number}: "
            f"{retry_state.outcome.exception()}"
        ),
    )
    def _call_api(self, model: str, prompt: str, max_tokens: int = 1000) -> str:
        """Call Claude API with retry."""
        message = self.client.messages.create(
            model=model,
            max_tokens=max_tokens,
            temperature=0.7,
            messages=[{"role": "user", "content": prompt}],
        )
        return message.content[0].text

    def classify_article(self, article: Dict) -> Optional[Dict]:
        """
        Classify if article is relevant for the channel.
        Uses Haiku for cost efficiency.

        Returns:
            Dict with {relevant: bool, confidence: int, category: str, format: str,
                       reason: str, needs_review: bool, url_check_needed: bool}
            or None if error
        """
        title = article.get("title", "")
        description = article.get("summary", "")[:500]

        prompt = f"""–¢—ã ‚Äî –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–ª—è Telegram-–∫–∞–Ω–∞–ª–∞ "AI –¥–ª—è –¥–æ–º–∞".

–¶–ï–õ–ï–í–ê–Ø –ê–£–î–ò–¢–û–†–ò–Ø (CONSUMER):
- –û–±—ã—á–Ω—ã–µ –ª—é–¥–∏, –ù–ï –ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç—ã –∏ –ù–ï –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω—ã–µ –∫–ª–∏–µ–Ω—Ç—ã
- –ò—Å–ø–æ–ª—å–∑—É—é—Ç AI –¥–ª—è –ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á: –¥–æ–º, —É—á—ë–±–∞, —Ç–≤–æ—Ä—á–µ—Å—Ç–≤–æ, –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—å
- –ò–Ω—Ç–µ—Ä–µ—Å—É–µ—Ç: –ø—Ä–æ—Å—Ç—ã–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã, –Ω–æ–≤—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏, –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Å–æ–≤–µ—Ç—ã
- –Ø–∑—ã–∫: –ø–æ–Ω—è—Ç–Ω—ã–π, –±–µ–∑ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–≥–æ –∂–∞—Ä–≥–æ–Ω–∞

–í–ö–õ–Æ–ß–ê–¢–¨ (relevant: true, audience: "consumer"):
- AI-–∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –¥–ª—è –æ–±—ã—á–Ω—ã—Ö –ª—é–¥–µ–π (ChatGPT, Claude, Midjourney, Canva AI)
- –ù–æ–≤–æ—Å—Ç–∏ AI-–∫–æ–º–ø–∞–Ω–∏–π —Å –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–π –ø–æ–ª—å–∑–æ–π –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
- –û–±–Ω–æ–≤–ª–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –µ—Å–ª–∏ —ç—Ç–æ –≤–ª–∏—è–µ—Ç –Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π (–Ω–æ–≤—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏, –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å)
- AI –¥–ª—è –¥–æ–º–∞, –±—ã—Ç–∞, —É—á—ë–±—ã, —Ç–≤–æ—Ä—á–µ—Å—Ç–≤–∞, –∑–¥–æ—Ä–æ–≤—å—è
- –ë–µ—Å–ø–ª–∞—Ç–Ω—ã–µ –∏ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã
- –°—Ä–∞–≤–Ω–µ–Ω–∏—è –¥–ª—è –æ–±—ã—á–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
- –ú–æ–±–∏–ª—å–Ω—ã–µ AI-–ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
- –ì–æ–ª–æ—Å–æ–≤—ã–µ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç—ã, —É–º–Ω—ã–π –¥–æ–º

–ò–°–ö–õ–Æ–ß–ê–¢–¨ (relevant: false) ‚Äî ENTERPRISE/B2B –∫–æ–Ω—Ç–µ–Ω—Ç:
- –†–µ—à–µ–Ω–∏—è –¥–ª—è –±–∏–∑–Ω–µ—Å–∞: "enterprise", "B2B", "corporate", "business solution"
- –ò–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–∞: "deployment", "infrastructure", "cloud", "MLOps", "kubernetes"
- –†–∞–∑—Ä–∞–±–æ—Ç–∫–∞: API, SDK, framework, library, open-source (–¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤)
- –ù–∞—É—á–Ω—ã–µ —Å—Ç–∞—Ç—å–∏: arxiv, research paper, benchmark, model architecture
- –§–∏–Ω–∞–Ω—Å—ã –±–µ–∑ –ø—Ä–æ–¥—É–∫—Ç–∞: "raises $X", "funding round", "valuation"
- HR/–Ω–∞–π–º: "hiring", "joins", "appointed"
- Security/compliance: "SOC2", "HIPAA", "enterprise security"
- –ê–Ω–∞–ª–∏—Ç–∏–∫–∞ —Ä—ã–Ω–∫–∞: "market share", "industry report"
- –ü–∞—Ä—Ç–Ω—ë—Ä—Å—Ç–≤–∞ B2B: "partnership with [enterprise company]"

–ü–†–ò–ú–ï–†–´:
‚úÖ "ChatGPT can now edit images" ‚Üí consumer, –ø—Ä–∞–∫—Ç–∏—á–Ω–æ
‚úÖ "New free AI tool for photo editing" ‚Üí consumer, –±–µ—Å–ø–ª–∞—Ç–Ω–æ
‚úÖ "Claude —Ç–µ–ø–µ—Ä—å –¥–æ—Å—Ç—É–ø–µ–Ω –≤ –º–æ–±–∏–ª—å–Ω–æ–º –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–∏" ‚Üí consumer
‚ùå "OpenAI launches enterprise API tier" ‚Üí enterprise
‚ùå "New ML framework for developers" ‚Üí developer
‚ùå "Anthropic raises $2B at $15B valuation" ‚Üí finance only
‚ùå "AWS announces AI infrastructure updates" ‚Üí enterprise
‚ùå "How to fine-tune LLaMA with LoRA" ‚Üí developer

EDGE-CASES:
- –ù–æ–≤–∞—è –º–æ–¥–µ–ª—å (GPT-5, Claude 4) ‚Üí –í–ö–õ–Æ–ß–ò–¢–¨ –µ—Å–ª–∏ –ø—Ä–æ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
- –ù–æ–≤–æ–µ API ‚Üí –ò–°–ö–õ–Æ–ß–ò–¢–¨ (–¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤)
- –¶–µ–Ω—ã —É–ø–∞–ª–∏ ‚Üí –í–ö–õ–Æ–ß–ò–¢–¨ –µ—Å–ª–∏ –¥–ª—è consumer
- –ù–∞—É—á–Ω—ã–π –ø—Ä–æ—Ä—ã–≤ ‚Üí –í–ö–õ–Æ–ß–ò–¢–¨ —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –ø–æ–Ω—è—Ç–Ω–æ

FALLBACK:
- –°–æ–º–Ω–µ–≤–∞–µ—à—å—Å—è –≤ –∞—É–¥–∏—Ç–æ—Ä–∏–∏ ‚Üí audience: "mixed", —Å–Ω–∏–∑—å confidence –Ω–∞ 15
- –ù–µ–ø–æ–Ω—è—Ç–Ω–æ enterprise –∏–ª–∏ consumer ‚Üí needs_review: true
- –ü—É—Å—Ç–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ ‚Üí confidence -= 20

–°–¢–ê–¢–¨–Ø:
–ó–∞–≥–æ–ª–æ–≤–æ–∫: {title}
–ò—Å—Ç–æ—á–Ω–∏–∫: {article.get('source', '')}
–û–ø–∏—Å–∞–Ω–∏–µ: {description}
–°—Å—ã–ª–∫–∞: {article.get('link', '')}

–û–ø—Ä–µ–¥–µ–ª–∏:
1. –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞ –¥–ª—è CONSUMER –∞—É–¥–∏—Ç–æ—Ä–∏–∏?
2. –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å (0-100)
3. –ö–∞—Ç–µ–≥–æ—Ä–∏—è (tool/news/update/trend/comparison/tip)
4. –ê—É–¥–∏—Ç–æ—Ä–∏—è (consumer/enterprise/mixed)
5. –ü—Ä–∏—á–∏–Ω–∞ (–∫—Ä–∞—Ç–∫–æ –Ω–∞ —Ä—É—Å—Å–∫–æ–º)

–û—Ç–≤–µ—Ç—å –¢–û–õ–¨–ö–û –≤–∞–ª–∏–¥–Ω—ã–º JSON –±–µ–∑ markdown:
{{"relevant": true/false, "confidence": 0-100, "category": "...", "audience": "consumer/enterprise/mixed", "reason": "...", "needs_review": false, "url_check_needed": false}}"""

        try:
            response = self._call_api(self.haiku_model, prompt, max_tokens=250)
            result = parse_classifier_response(response)

            # Log classification result
            if result.get("needs_review"):
                logger.info(
                    f"Needs review: {title[:50]}... "
                    f"(confidence: {result.get('confidence')}, reason: {result.get('reason')})"
                )

            return result
        except Exception as e:
            logger.error(f"Error classifying article: {e}")
            return None

    def generate_post(self, article: Dict, post_format: PostFormat = None) -> Optional[GeneratedPost]:
        """
        Generate a post from article using universal long-form format.
        Uses Sonnet for quality.

        Note: post_format is kept for backward compatibility but not used.
        All posts now use the same universal format (700-900 chars).
        """
        prompt = self._get_universal_prompt(article)

        try:
            # Increased max_tokens for longer posts
            response = self._call_api(self.sonnet_model, prompt, max_tokens=1500)

            # Parse response (expecting JSON with text and image_prompt)
            try:
                # Clean markdown code blocks if present
                cleaned = response.strip()
                cleaned = re.sub(r"^```json\s*", "", cleaned, flags=re.IGNORECASE)
                cleaned = re.sub(r"^```\s*", "", cleaned)
                cleaned = re.sub(r"\s*```$", "", cleaned)

                # Try to find JSON object in response with nested braces support
                json_match = re.search(r'\{.*?"text"\s*:\s*".*?\}', cleaned, re.DOTALL)
                if json_match:
                    # Extract the full JSON object including nested content
                    brace_count = 0
                    start_idx = json_match.start()
                    for i, char in enumerate(cleaned[start_idx:], start=start_idx):
                        if char == '{':
                            brace_count += 1
                        elif char == '}':
                            brace_count -= 1
                            if brace_count == 0:
                                cleaned = cleaned[start_idx:i+1]
                                break

                data = json.loads(cleaned)
                text = data.get("text")
                image_prompt = data.get("image_prompt")

                # Fallback if text extraction failed
                if not text:
                    logger.warning("JSON parsed but 'text' field empty, using raw response")
                    text = response

            except (json.JSONDecodeError, TypeError, ValueError) as e:
                logger.warning(f"Failed to parse post JSON: {e}, using raw response")
                text = response
                image_prompt = None

            # Validate and fix HTML before returning
            text = validate_telegram_html(text)

            return GeneratedPost(
                text=text,
                format=post_format or PostFormat.AI_TOOL,
                article_url=article.get("link", ""),
                article_title=article.get("title", ""),
                image_prompt=image_prompt,
                image_url=article.get("image_url"),  # OG/RSS image from article
            )
        except Exception as e:
            logger.error(f"Error generating post: {e}")
            return None

    def _get_universal_prompt(self, article: Dict) -> str:
        """
        Universal prompt for long-form posts (1500-2000 chars).
        Style: Databoard-inspired, links embedded in text.
        """
        article_link = article.get('link', '')
        source_name = article.get('source', '–∏—Å—Ç–æ—á–Ω–∏–∫')

        return f"""–¢—ã ‚Äî –∫–æ–ø–∏—Ä–∞–π—Ç–µ—Ä Telegram-–∫–∞–Ω–∞–ª–∞ "AI –¥–ª—è –¥–æ–º–∞" (@ai_dlya_doma).

–ê–£–î–ò–¢–û–†–ò–Ø:
- –í—Å–µ, –∫—Ç–æ –∏–Ω—Ç–µ—Ä–µ—Å—É–µ—Ç—Å—è AI –∏ —Ö–æ—á–µ—Ç –ø—Ä–∏–º–µ–Ω—è—Ç—å –µ–≥–æ –≤ –∂–∏–∑–Ω–∏
- –ù–µ —Ç–æ–ª—å–∫–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç—ã ‚Äî –æ–±—ã—á–Ω—ã–µ –ª—é–¥–∏ —Ç–æ–∂–µ
- –¶–µ–Ω—è—Ç: –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å, –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫—É—é –ø–æ–ª—å–∑—É, –∫–æ–Ω–∫—Ä–µ—Ç–∏–∫—É

–°–¢–ò–õ–¨:
- –ò–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–π, —ç–Ω–µ—Ä–≥–∏—á–Ω—ã–π, –Ω–æ –±–µ–∑ –∫–ª–∏–∫–±–µ–π—Ç–∞
- –≠–º–æ–¥–∑–∏: 1-2 —à—Ç—É–∫–∏, —Ç–æ–ª—å–∫–æ –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ
- –û–±—Ä–∞—â–µ–Ω–∏–µ: –Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–µ –∏–ª–∏ –Ω–∞ "–≤—ã"
- –°—Å—ã–ª–∫–∏ –í–ù–£–¢–†–ò —Ç–µ–∫—Å—Ç–∞, –Ω–µ –≤ –∫–æ–Ω—Ü–µ
- –î–ª–∏–Ω–∞: 700-900 —Å–∏–º–≤–æ–ª–æ–≤ (—ç—Ç–æ –≤–∞–∂–Ω–æ –¥–ª—è Telegram caption!)

–°–¢–†–£–ö–¢–£–†–ê –ü–û–°–¢–ê:
```
[–≠–º–æ–¥–∑–∏] <b>–ó–∞–≥–æ–ª–æ–≤–æ–∫ ‚Äî –∫–æ—Ä–æ—Ç–∫–∏–π, —Ü–µ–ø–ª—è—é—â–∏–π</b>

–ü–µ—Ä–≤—ã–π –∞–±–∑–∞—Ü ‚Äî —Å—É—Ç—å –Ω–æ–≤–æ—Å—Ç–∏. –û —á—ë–º —Ä–µ—á—å, –ø–æ—á–µ–º—É —ç—Ç–æ –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ. 2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è.

–í—Ç–æ—Ä–æ–π –∞–±–∑–∞—Ü ‚Äî –¥–µ—Ç–∞–ª–∏. –ß—Ç–æ –∏–º–µ–Ω–Ω–æ –∏–∑–º–µ–Ω–∏–ª–æ—Å—å, –∫–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç, –∫–∞–∫–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏. –ó–¥–µ—Å—å –º–æ–∂–Ω–æ <a href="URL">–≤—Å—Ç–∞–≤–∏—Ç—å —Å—Å—ã–ª–∫—É</a> –Ω–∞ –∏—Å—Ç–æ—á–Ω–∏–∫ –∏–ª–∏ —Å–∞–º —Å–µ—Ä–≤–∏—Å.

–¢—Ä–µ—Ç–∏–π –∞–±–∑–∞—Ü ‚Äî –∫–æ–Ω—Ç–µ–∫—Å—Ç. –ü–æ—á–µ–º—É —ç—Ç–æ –≤–∞–∂–Ω–æ, –∫–∞–∫ —ç—Ç–æ –≤–ª–∏—è–µ—Ç –Ω–∞ —Ä—ã–Ω–æ–∫/–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π, —á—Ç–æ –¥—É–º–∞—é—Ç —ç–∫—Å–ø–µ—Ä—Ç—ã.

[–û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ] –ß–µ—Ç–≤—ë—Ä—Ç—ã–π –∞–±–∑–∞—Ü ‚Äî –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–≤–æ–¥ –∏–ª–∏ –≤–æ–ø—Ä–æ—Å –∫ —á–∏—Ç–∞—Ç–µ–ª—è–º.
```

–ê–ù–¢–ò-–ü–ê–¢–¢–ï–†–ù–´ (–ù–ò–ö–û–ì–î–ê –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–π):
- –®–∞–±–ª–æ–Ω–Ω—ã–µ CTA —Ç–∏–ø–∞ "üëâ –ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å", "üîó –°—Å—ã–ª–∫–∞"
- –ì–æ–ª—ã–µ URL –±–µ–∑ <a> —Ç–µ–≥–æ–≤
- "–ù–µ–π—Ä–æ—Å–µ—Ç—å" ‚Üí –∏—Å–ø–æ–ª—å–∑—É–π "AI" –∏–ª–∏ –Ω–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
- "–†–µ–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—ã–π", "—É–Ω–∏–∫–∞–ª—å–Ω—ã–π", "–ø—Ä–æ—Ä—ã–≤–Ω–æ–π"
- –ù–∞—á–∞–ª–æ —Å "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ–º...", "–í—Å—Ç—Ä–µ—á–∞–π—Ç–µ..."
- –°–ø–∏—Å–æ–∫ —Ñ–∏—á —á–µ—Ä–µ–∑ –±—É–ª–ª–µ—Ç—ã –≤ –∫–∞–∂–¥–æ–º –ø–æ—Å—Ç–µ
- –ü—É—Å—Ç—ã–µ –æ–±–µ—â–∞–Ω–∏—è –±–µ–∑ –∫–æ–Ω–∫—Ä–µ—Ç–∏–∫–∏

–ü–†–ê–í–ò–õ–ê –°–°–´–õ–û–ö:
- –°—Å—ã–ª–∫–∏ –≤—Å—Ç—Ä–∞–∏–≤–∞–π –í –¢–ï–ö–°–¢: <a href="URL">—á–∏—Ç–∞–π—Ç–µ –ø–æ–¥—Ä–æ–±–Ω–µ–µ</a>
- –¢–µ–∫—Å—Ç —Å—Å—ã–ª–∫–∏ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–º: "–ø–∏—à–µ—Ç TechCrunch", "—Å–æ–æ–±—â–∞–µ—Ç –∫–æ–º–ø–∞–Ω–∏—è"
- –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å 1-2 —Å—Å—ã–ª–∫–∏, –Ω–µ –±–æ–ª—å—à–µ
- –û—Å–Ω–æ–≤–Ω–∞—è —Å—Å—ã–ª–∫–∞ –Ω–∞ –∏—Å—Ç–æ—á–Ω–∏–∫: {article_link}

–°–¢–ê–¢–¨–Ø –î–õ–Ø –û–ë–†–ê–ë–û–¢–ö–ò:
–ó–∞–≥–æ–ª–æ–≤–æ–∫: {article.get('title', '')}
–ò—Å—Ç–æ—á–Ω–∏–∫: {source_name}
–û–ø–∏—Å–∞–Ω–∏–µ: {article.get('summary', '')[:800]}
–°—Å—ã–ª–∫–∞: {article_link}

–û—Ç–≤–µ—Ç –¢–û–õ–¨–ö–û JSON –±–µ–∑ markdown –±–ª–æ–∫–æ–≤:
{{"text": "–≥–æ—Ç–æ–≤—ã–π –ø–æ—Å—Ç —Å HTML-—Ä–∞–∑–º–µ—Ç–∫–æ–π, 700-900 —Å–∏–º–≤–æ–ª–æ–≤", "image_prompt": "DALL-E prompt in English, tech illustration style, modern, clean, 40 words max"}}"""


    def generate_post_for_rubric(
        self, article: Dict, rubric_name: str
    ) -> Optional[GeneratedPost]:
        """
        Generate a post for a specific rubric using its template.

        Args:
            article: Article data
            rubric_name: Name of the rubric (e.g., 'tool_review', 'news')

        Returns:
            GeneratedPost or None
        """
        try:
            from rubrics import Rubric, RUBRIC_PROMPTS

            # Get rubric enum
            try:
                rubric = Rubric(rubric_name)
            except ValueError:
                logger.warning(f"Unknown rubric: {rubric_name}, using default")
                return self.generate_post(article)

            # Get rubric-specific prompt template
            rubric_template = RUBRIC_PROMPTS.get(rubric)
            if not rubric_template:
                logger.warning(f"No template for rubric: {rubric_name}, using default")
                return self.generate_post(article)

            # Build full prompt
            article_link = article.get('link', '')
            source_name = article.get('source', '–∏—Å—Ç–æ—á–Ω–∏–∫')

            prompt = f"""–¢—ã ‚Äî –∫–æ–ø–∏—Ä–∞–π—Ç–µ—Ä Telegram-–∫–∞–Ω–∞–ª–∞ "AI –¥–ª—è –¥–æ–º–∞" (@ai_dlya_doma).

–ê–£–î–ò–¢–û–†–ò–Ø: –í—Å–µ, –∫—Ç–æ –∏–Ω—Ç–µ—Ä–µ—Å—É–µ—Ç—Å—è AI ‚Äî –æ—Ç –Ω–æ–≤–∏—á–∫–æ–≤ –¥–æ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö.

–†–£–ë–†–ò–ö–ê –ò –§–û–†–ú–ê–¢:
{rubric_template}

–°–¢–ê–¢–¨–Ø –î–õ–Ø –û–ë–†–ê–ë–û–¢–ö–ò:
–ó–∞–≥–æ–ª–æ–≤–æ–∫: {article.get('title', '')}
–ò—Å—Ç–æ—á–Ω–∏–∫: {source_name}
–û–ø–∏—Å–∞–Ω–∏–µ: {article.get('summary', '')[:800]}
–°—Å—ã–ª–∫–∞: {article_link}

–ü–†–ê–í–ò–õ–ê:
- –ò—Å–ø–æ–ª—å–∑—É–π HTML-—Ä–∞–∑–º–µ—Ç–∫—É Telegram: <b>, <i>, <a href="">
- –°—Å—ã–ª–∫–∏ –≤—Å—Ç—Ä–∞–∏–≤–∞–π –≤ —Ç–µ–∫—Å—Ç
- –î–ª–∏–Ω–∞: 700-900 —Å–∏–º–≤–æ–ª–æ–≤
- –ó–∞–º–µ–Ω–∏ [URL] –Ω–∞ —Ä–µ–∞–ª—å–Ω—É—é —Å—Å—ã–ª–∫—É: {article_link}

–û—Ç–≤–µ—Ç –¢–û–õ–¨–ö–û JSON:
{{"text": "–≥–æ—Ç–æ–≤—ã–π –ø–æ—Å—Ç", "image_prompt": "DALL-E prompt, 40 words"}}"""

            response = self._call_api(self.sonnet_model, prompt, max_tokens=1500)

            # Parse response
            try:
                cleaned = response.strip()
                cleaned = re.sub(r"^```json\s*", "", cleaned, flags=re.IGNORECASE)
                cleaned = re.sub(r"^```\s*", "", cleaned)
                cleaned = re.sub(r"\s*```$", "", cleaned)

                data = json.loads(cleaned)
                text = data.get("text", response)
                image_prompt = data.get("image_prompt")
            except (json.JSONDecodeError, TypeError, ValueError):
                text = response
                image_prompt = None

            text = validate_telegram_html(text)

            # Map rubric to PostFormat
            format_map = {
                "tool_review": PostFormat.AI_TOOL,
                "news": PostFormat.AI_TOOL,
                "prompt_home": PostFormat.PROMPT_DAY,
                "lifehack": PostFormat.QUICK_TIP,
                "free_service": PostFormat.AI_TOOL,
                "collection": PostFormat.CHECKLIST,
                "digest": PostFormat.CHECKLIST,
            }
            post_format = format_map.get(rubric_name, PostFormat.AI_TOOL)

            return GeneratedPost(
                text=text,
                format=post_format,
                article_url=article.get("link", ""),
                article_title=article.get("title", ""),
                image_prompt=image_prompt,
                image_url=article.get("image_url"),
            )

        except Exception as e:
            logger.error(f"Error generating post for rubric {rubric_name}: {e}")
            return None

    def generate_image_prompt(self, post: GeneratedPost) -> str:
        """
        Generate DALL-E prompt for post image.
        Uses Haiku for cost efficiency.
        """
        if post.image_prompt:
            return post.image_prompt

        prompt = f"""Create a DALL-E 3 image prompt for this Telegram post:

POST:
{post.text[:300]}

STYLE REQUIREMENTS:
- Flat design with soft gradients
- Pastel colors: light blue, pink, mint, lavender
- Minimalist icons
- Isometric perspective
- NO text on image
- NO people faces
- Cozy, friendly feeling
- Modern, clean look

Format: 1024x1024, English, 50-80 words.

Respond with ONLY the prompt, no explanations."""

        try:
            return self._call_api(self.haiku_model, prompt, max_tokens=150)
        except Exception as e:
            logger.error(f"Error generating image prompt: {e}")
            return "Flat design illustration, pastel colors, minimalist icons, cozy modern aesthetic, soft gradients, no text"

    def filter_and_rank_articles(
        self, articles: List[Dict], max_posts: int = 5
    ) -> List[tuple]:
        """
        Filter relevant articles and rank by confidence.

        Returns:
            List of (article, classification) tuples, sorted by confidence
        """
        classified = []

        for article in articles:
            result = self.classify_article(article)
            if result and result.get("relevant") and result.get("confidence", 0) >= 45:
                classified.append((article, result))
                logger.info(
                    f"Relevant: {article.get('title', '')[:50]}... "
                    f"(confidence: {result.get('confidence')})"
                )

        # Sort by confidence, take top N
        classified.sort(key=lambda x: x[1].get("confidence", 0), reverse=True)
        return classified[:max_posts]

    def generate_daily_posts(
        self, articles: List[Dict], count: int = 5
    ) -> List[GeneratedPost]:
        """
        Generate posts for the day from articles.

        Args:
            articles: List of news articles
            count: Number of posts to generate

        Returns:
            List of GeneratedPost objects
        """
        logger.info(f"Generating {count} posts from {len(articles)} articles")

        # Filter and rank articles
        ranked = self.filter_and_rank_articles(articles, max_posts=count)

        if not ranked:
            logger.warning("No relevant articles found")
            return []

        posts = []
        for article, classification in ranked:
            format_str = classification.get("format", "ai_tool")
            try:
                post_format = PostFormat(format_str)
            except ValueError:
                post_format = PostFormat.AI_TOOL

            post = self.generate_post(article, post_format)
            if post:
                # Generate image prompt if not present
                if not post.image_prompt:
                    post.image_prompt = self.generate_image_prompt(post)
                posts.append(post)
                logger.info(f"Generated post: {post.format.value}")

        return posts


if __name__ == "__main__":
    from dotenv import load_dotenv
    load_dotenv()

    generator = PostGenerator()

    # Test with dummy article
    test_article = {
        "title": "Canva launches AI photo editor for Instagram",
        "source": "TechCrunch",
        "summary": "Canva announced a new AI-powered photo editor that can automatically enhance photos, remove backgrounds, and suggest Instagram-ready filters. The tool is free for basic use.",
        "link": "https://example.com/canva-ai",
    }

    print("Testing classification...")
    result = generator.classify_article(test_article)
    print(f"Classification: {result}")

    if result and result.get("relevant"):
        print("\nGenerating post...")
        post = generator.generate_post(test_article, PostFormat.AI_TOOL)
        if post:
            print(f"\n{post.text}")
            print(f"\nImage prompt: {post.image_prompt}")
