"""Generate individual posts from news articles for @ai_dlya_doma channel."""

import json
import os
import re
from dataclasses import dataclass
from enum import Enum
from typing import Dict, List, Optional, Tuple

from anthropic import (
    Anthropic,
    APIConnectionError,
    APITimeoutError,
    RateLimitError,
)
from openai import OpenAI
from tenacity import (
    retry,
    retry_if_exception_type,
    stop_after_attempt,
    wait_exponential,
)

from logger import get_logger

logger = get_logger("news_bot.post_generator")


def parse_classifier_response(response_text: str) -> dict:
    """
    Parse classifier response with error handling.
    Returns default response if LLM returned invalid JSON.
    """
    DEFAULT_RESPONSE = {
        "relevant": False,
        "confidence": 0,
        "category": "parse_error",
        "format": "ai_tool",
        "reason": "Failed to parse LLM response",
        "needs_review": True,
        "url_check_needed": True,
    }

    try:
        # Remove markdown code blocks if present
        cleaned = re.sub(r"^```json\s*", "", response_text.strip())
        cleaned = re.sub(r"\s*```$", "", cleaned)

        # Try to find JSON in text
        json_match = re.search(r"\{[^{}]*\}", cleaned, re.DOTALL)
        if json_match:
            cleaned = json_match.group()

        data = json.loads(cleaned)

        # Validate required fields
        if "relevant" not in data or "confidence" not in data:
            data = DEFAULT_RESPONSE.copy()
            data["reason"] = "Missing required fields"
            return data

        # Normalize confidence to 0-100
        data["confidence"] = max(0, min(100, int(data.get("confidence", 0))))

        # Defaults for optional fields
        data.setdefault("category", "unknown")
        data.setdefault("format", "ai_tool")
        data.setdefault("reason", "")
        data.setdefault("needs_review", data["confidence"] < 70)
        data.setdefault("url_check_needed", False)

        return data

    except (json.JSONDecodeError, TypeError, ValueError) as e:
        response = DEFAULT_RESPONSE.copy()
        response["reason"] = f"JSON parse error: {str(e)[:50]}"
        return response


def validate_telegram_html(text: str) -> str:
    """
    Validate and fix common HTML issues for Telegram.

    Telegram supports: <b>, <i>, <u>, <s>, <code>, <pre>, <a href="">
    """
    if not text:
        return text

    # Allowed Telegram HTML tags
    allowed_tags = ['b', 'i', 'u', 's', 'code', 'pre', 'a']

    # Count open and close tags
    for tag in allowed_tags:
        open_count = len(re.findall(rf'<{tag}[^>]*>', text, re.IGNORECASE))
        close_count = len(re.findall(rf'</{tag}>', text, re.IGNORECASE))

        # If imbalanced, try to fix or remove
        if open_count != close_count:
            logger.warning(f"HTML tag <{tag}> imbalanced: {open_count} open, {close_count} close")
            # Remove all instances of this tag if imbalanced
            text = re.sub(rf'<{tag}[^>]*>', '', text, flags=re.IGNORECASE)
            text = re.sub(rf'</{tag}>', '', text, flags=re.IGNORECASE)

    # Fix common LLM mistakes with <a> tags
    # Fix: <a href = "url"> ‚Üí <a href="url">
    text = re.sub(r'<a\s+href\s*=\s*["\']([^"\']+)["\']>', r'<a href="\1">', text)

    # Fix: missing quotes around href
    text = re.sub(r'<a\s+href=([^"\'\s>]+)>', r'<a href="\1">', text)

    # Remove any unsupported HTML tags
    text = re.sub(r'<(?!/?(?:b|i|u|s|code|pre|a)[^>]*>)[^>]+>', '', text)

    return text.strip()


def is_good_image(url: str) -> bool:
    """
    Check if OG-image URL is likely a good quality image.
    Returns False for placeholders, logos, icons, etc.
    """
    if not url:
        return False

    url_lower = url.lower()

    # Bad patterns - likely placeholders or low-quality images
    bad_patterns = [
        'placeholder', 'default', 'logo', 'icon', 'avatar',
        '1x1', '1x1.', 'pixel', 'blank', 'empty', 'spacer',
        'og-default', 'social-default', 'share-default',
        'no-image', 'noimage', 'missing'
    ]

    if any(pattern in url_lower for pattern in bad_patterns):
        return False

    # Check for valid image extension
    valid_extensions = ['.jpg', '.jpeg', '.png', '.webp', '.gif']
    has_valid_ext = any(url_lower.endswith(ext) or f'{ext}?' in url_lower
                        for ext in valid_extensions)

    # Some URLs don't have extensions but are still valid (e.g., CDN URLs)
    # Accept them if they don't match bad patterns
    if not has_valid_ext:
        # Check for common image CDN patterns
        cdn_patterns = ['cloudinary', 'imgix', 'cloudfront', 'akamai',
                        'fastly', 'cdn', 'images', 'media', 'assets']
        if not any(cdn in url_lower for cdn in cdn_patterns):
            return False

    return True


def generate_image_via_openai(prompt: str) -> Optional[str]:
    """
    Generate image using OpenAI DALL-E 3.
    Returns URL of generated image or None on error.
    """
    api_key = os.getenv('OPENAI_API_KEY')
    if not api_key:
        logger.warning("OPENAI_API_KEY not set, cannot generate image")
        return None

    try:
        client = OpenAI(api_key=api_key)
        response = client.images.generate(
            model="dall-e-3",
            prompt=prompt,
            size="1024x1024",
            quality="standard",
            n=1
        )
        return response.data[0].url
    except Exception as e:
        logger.error(f"OpenAI image generation failed: {e}")
        return None


def get_image_for_post(article: Dict, image_prompt: str = None) -> Tuple[Optional[str], str]:
    """
    Get image for post using hybrid approach:
    1. Try OG-image from article if it's good quality
    2. Fall back to OpenAI generation

    Returns:
        Tuple of (image_url, source_type)
        source_type: 'og_image', 'generated', or 'none'
    """
    # Try OG-image first
    og_image = article.get('image_url')
    if og_image and is_good_image(og_image):
        logger.info(f"Using OG-image: {og_image[:50]}...")
        return (og_image, 'og_image')

    # Generate if we have a prompt
    if image_prompt:
        logger.info("OG-image not suitable, generating via OpenAI...")
        generated_url = generate_image_via_openai(image_prompt)
        if generated_url:
            return (generated_url, 'generated')

    logger.warning("No image available for post")
    return (None, 'none')


class PostFormat(Enum):
    """Types of posts for the channel."""
    AI_TOOL = "ai_tool"          # AI-–Ω–∞—Ö–æ–¥–∫–∞ –¥–Ω—è
    QUICK_TIP = "quick_tip"      # –ë—ã—Å—Ç—Ä—ã–π —Å–æ–≤–µ—Ç
    PROMPT_DAY = "prompt_day"    # –ü—Ä–æ–º—Ç –¥–Ω—è
    COMPARISON = "comparison"    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ
    CHECKLIST = "checklist"      # –ß–µ–∫-–ª–∏—Å—Ç


@dataclass
class GeneratedPost:
    """A generated post ready for publication."""
    text: str
    format: PostFormat
    article_url: str
    article_title: str
    image_prompt: Optional[str] = None
    image_url: Optional[str] = None  # OG/RSS image URL from article


class PostGenerator:
    """Generate beautiful posts for Telegram channel."""

    def __init__(self, api_key: str = None):
        """Initialize with Anthropic API."""
        self.api_key = api_key or os.getenv("ANTHROPIC_API_KEY")
        if not self.api_key:
            raise ValueError("ANTHROPIC_API_KEY not found")

        self.client = Anthropic(api_key=self.api_key)
        self.haiku_model = "claude-3-haiku-20240307"
        self.sonnet_model = "claude-sonnet-4-20250514"

    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=2, min=4, max=60),
        retry=retry_if_exception_type(
            (RateLimitError, APIConnectionError, APITimeoutError)
        ),
        before_sleep=lambda retry_state: logger.warning(
            f"Claude API retry {retry_state.attempt_number}: "
            f"{retry_state.outcome.exception()}"
        ),
    )
    def _call_api(self, model: str, prompt: str, max_tokens: int = 1000) -> str:
        """Call Claude API with retry."""
        message = self.client.messages.create(
            model=model,
            max_tokens=max_tokens,
            temperature=0.7,
            messages=[{"role": "user", "content": prompt}],
        )
        return message.content[0].text

    def classify_article(self, article: Dict) -> Optional[Dict]:
        """
        Classify if article is relevant for the channel.
        Uses Haiku for cost efficiency.

        Returns:
            Dict with {relevant: bool, confidence: int, category: str, format: str,
                       reason: str, needs_review: bool, url_check_needed: bool}
            or None if error
        """
        title = article.get("title", "")
        description = article.get("summary", "")[:500]

        prompt = f"""–¢—ã ‚Äî –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–ª—è Telegram-–∫–∞–Ω–∞–ª–∞ "AI –¥–ª—è –¥–æ–º–∞".

–¶–ï–õ–ï–í–ê–Ø –ê–£–î–ò–¢–û–†–ò–Ø:
- –í—Å–µ, –∫—Ç–æ –∏–Ω—Ç–µ—Ä–µ—Å—É–µ—Ç—Å—è AI –∏ —Ö–æ—á–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –µ–≥–æ –≤ –∂–∏–∑–Ω–∏
- –ù–µ —Ç–æ–ª—å–∫–æ —Ç–µ—Ö–Ω–∞—Ä–∏ ‚Äî –æ–±—ã—á–Ω—ã–µ –ª—é–¥–∏ —Ç–æ–∂–µ
- –ò–Ω—Ç–µ—Ä–µ—Å—É–µ—Ç: —á—Ç–æ –Ω–æ–≤–æ–≥–æ –≤ –º–∏—Ä–µ AI, –∫–∞–∫–∏–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –ø–æ—è–≤–∏–ª–∏—Å—å, —á—Ç–æ –∏–∑–º–µ–Ω–∏–ª–æ—Å—å

–í–ö–õ–Æ–ß–ê–¢–¨ (relevant: true):
- AI-–∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –ª—é–±–æ–≥–æ —Ç–∏–ø–∞ (ChatGPT, Claude, Midjourney, Runway –∏ –¥—Ä.)
- –ù–æ–≤–æ—Å—Ç–∏ AI-–∫–æ–º–ø–∞–Ω–∏–π (OpenAI, Anthropic, Google, Meta, Microsoft)
- –û–±–Ω–æ–≤–ª–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π (GPT-5, Claude 4, Gemini 2, Llama 4 –∏ –¥—Ä.)
- –¢—Ä–µ–Ω–¥—ã –≤ AI: —á—Ç–æ –º–µ–Ω—è–µ—Ç—Å—è, –∫—É–¥–∞ –¥–≤–∏–∂–µ—Ç—Å—è –∏–Ω–¥—É—Å—Ç—Ä–∏—è
- –ò–Ω—Ç–µ—Ä–µ—Å–Ω—ã–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è AI (–∫–µ–π—Å—ã, –ø—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è)
- –°—Ä–∞–≤–Ω–µ–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤
- AI –≤ –ø–æ–≤—Å–µ–¥–Ω–µ–≤–Ω–æ–π –∂–∏–∑–Ω–∏
- –ù–æ–≤—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Å–µ—Ä–≤–∏—Å–∞—Ö

–ò–°–ö–õ–Æ–ß–ê–¢–¨ (relevant: false):
- –ß–∏—Å—Ç–æ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è (API docs, SDK reference)
- –ö–æ–¥, —Ç—É—Ç–æ—Ä–∏–∞–ª—ã –¥–ª—è –ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç–æ–≤
- –¢–æ–ª—å–∫–æ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –Ω–æ–≤–æ—Å—Ç–∏ (–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏ –±–µ–∑ –ø—Ä–æ–¥—É–∫—Ç–∞/—Ñ—É–Ω–∫—Ü–∏–∏)
- –°—Ç–∞—Ç—å–∏ —Å—Ç–∞—Ä—à–µ 7 –¥–Ω–µ–π (–ø—Ä–æ–≤–µ—Ä—å –¥–∞—Ç—É –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω–∞)
- –ö—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–∞, NFT, blockchain (–µ—Å–ª–∏ –Ω–µ —Å–≤—è–∑–∞–Ω–æ —Å AI)
- –ù–∞—É—á–Ω—ã–µ —Å—Ç–∞—Ç—å–∏ –±–µ–∑ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–π –ø–æ–ª—å–∑—ã
- –í–∞–∫–∞–Ω—Å–∏–∏, –Ω–∞–π–º

EDGE-CASES:
- "OpenAI raises $10B" ‚Üí –ò–°–ö–õ–Æ–ß–ò–¢–¨ (—Ç–æ–ª—å–∫–æ —Ñ–∏–Ω–∞–Ω—Å—ã)
- "OpenAI launches new feature" ‚Üí –í–ö–õ–Æ–ß–ò–¢–¨
- "How to build AI agent" (–¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤) ‚Üí –ò–°–ö–õ–Æ–ß–ò–¢–¨
- "Best AI tools for 2025" ‚Üí –í–ö–õ–Æ–ß–ò–¢–¨
- "New Claude model" ‚Üí –í–ö–õ–Æ–ß–ò–¢–¨
- –ü—É—Å—Ç–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ ‚Üí —Å–Ω–∏–∑—å confidence –Ω–∞ 20

FALLBACK:
- –ù–µ —É–≤–µ—Ä–µ–Ω ‚Üí confidence < 70
- –ù–∞ –≥—Ä–∞–Ω–∏ ‚Üí relevant: true, confidence: 55-65, needs_review: true
- –ù–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ ‚Üí category: "news"

–°–¢–ê–¢–¨–Ø:
–ó–∞–≥–æ–ª–æ–≤–æ–∫: {title}
–ò—Å—Ç–æ—á–Ω–∏–∫: {article.get('source', '')}
–û–ø–∏—Å–∞–Ω–∏–µ: {description}
–°—Å—ã–ª–∫–∞: {article.get('link', '')}

–û–ø—Ä–µ–¥–µ–ª–∏:
1. –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞ –ª–∏ —Å—Ç–∞—Ç—å—è?
2. –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å (0-100)
3. –ö–∞—Ç–µ–≥–æ—Ä–∏—è (tool/news/update/trend/comparison/tip)
4. –ü—Ä–∏—á–∏–Ω–∞ (–∫—Ä–∞—Ç–∫–æ –Ω–∞ —Ä—É—Å—Å–∫–æ–º)

–û—Ç–≤–µ—Ç—å –¢–û–õ–¨–ö–û –≤–∞–ª–∏–¥–Ω—ã–º JSON –±–µ–∑ markdown:
{{"relevant": true/false, "confidence": 0-100, "category": "...", "reason": "...", "needs_review": false, "url_check_needed": false}}"""

        try:
            response = self._call_api(self.haiku_model, prompt, max_tokens=250)
            result = parse_classifier_response(response)

            # Log classification result
            if result.get("needs_review"):
                logger.info(
                    f"Needs review: {title[:50]}... "
                    f"(confidence: {result.get('confidence')}, reason: {result.get('reason')})"
                )

            return result
        except Exception as e:
            logger.error(f"Error classifying article: {e}")
            return None

    def generate_post(self, article: Dict, post_format: PostFormat = None) -> Optional[GeneratedPost]:
        """
        Generate a post from article using universal long-form format.
        Uses Sonnet for quality.

        Note: post_format is kept for backward compatibility but not used.
        All posts now use the same universal format (700-900 chars).
        """
        prompt = self._get_universal_prompt(article)

        try:
            # Increased max_tokens for longer posts
            response = self._call_api(self.sonnet_model, prompt, max_tokens=1500)

            # Parse response (expecting JSON with text and image_prompt)
            try:
                # Clean markdown code blocks if present
                cleaned = response.strip()
                cleaned = re.sub(r"^```json\s*", "", cleaned, flags=re.IGNORECASE)
                cleaned = re.sub(r"^```\s*", "", cleaned)
                cleaned = re.sub(r"\s*```$", "", cleaned)

                # Try to find JSON object in response with nested braces support
                json_match = re.search(r'\{.*?"text"\s*:\s*".*?\}', cleaned, re.DOTALL)
                if json_match:
                    # Extract the full JSON object including nested content
                    brace_count = 0
                    start_idx = json_match.start()
                    for i, char in enumerate(cleaned[start_idx:], start=start_idx):
                        if char == '{':
                            brace_count += 1
                        elif char == '}':
                            brace_count -= 1
                            if brace_count == 0:
                                cleaned = cleaned[start_idx:i+1]
                                break

                data = json.loads(cleaned)
                text = data.get("text")
                image_prompt = data.get("image_prompt")

                # Fallback if text extraction failed
                if not text:
                    logger.warning("JSON parsed but 'text' field empty, using raw response")
                    text = response

            except (json.JSONDecodeError, TypeError, ValueError) as e:
                logger.warning(f"Failed to parse post JSON: {e}, using raw response")
                text = response
                image_prompt = None

            # Validate and fix HTML before returning
            text = validate_telegram_html(text)

            return GeneratedPost(
                text=text,
                format=post_format or PostFormat.AI_TOOL,
                article_url=article.get("link", ""),
                article_title=article.get("title", ""),
                image_prompt=image_prompt,
                image_url=article.get("image_url"),  # OG/RSS image from article
            )
        except Exception as e:
            logger.error(f"Error generating post: {e}")
            return None

    def _get_universal_prompt(self, article: Dict) -> str:
        """
        Universal prompt for long-form posts (1500-2000 chars).
        Style: Databoard-inspired, links embedded in text.
        """
        article_link = article.get('link', '')
        source_name = article.get('source', '–∏—Å—Ç–æ—á–Ω–∏–∫')

        return f"""–¢—ã ‚Äî –∫–æ–ø–∏—Ä–∞–π—Ç–µ—Ä Telegram-–∫–∞–Ω–∞–ª–∞ "AI –¥–ª—è –¥–æ–º–∞" (@ai_dlya_doma).

–ê–£–î–ò–¢–û–†–ò–Ø:
- –í—Å–µ, –∫—Ç–æ –∏–Ω—Ç–µ—Ä–µ—Å—É–µ—Ç—Å—è AI –∏ —Ö–æ—á–µ—Ç –ø—Ä–∏–º–µ–Ω—è—Ç—å –µ–≥–æ –≤ –∂–∏–∑–Ω–∏
- –ù–µ —Ç–æ–ª—å–∫–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç—ã ‚Äî –æ–±—ã—á–Ω—ã–µ –ª—é–¥–∏ —Ç–æ–∂–µ
- –¶–µ–Ω—è—Ç: –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å, –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫—É—é –ø–æ–ª—å–∑—É, –∫–æ–Ω–∫—Ä–µ—Ç–∏–∫—É

–°–¢–ò–õ–¨:
- –ò–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–π, —ç–Ω–µ—Ä–≥–∏—á–Ω—ã–π, –Ω–æ –±–µ–∑ –∫–ª–∏–∫–±–µ–π—Ç–∞
- –≠–º–æ–¥–∑–∏: 1-2 —à—Ç—É–∫–∏, —Ç–æ–ª—å–∫–æ –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ
- –û–±—Ä–∞—â–µ–Ω–∏–µ: –Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–µ –∏–ª–∏ –Ω–∞ "–≤—ã"
- –°—Å—ã–ª–∫–∏ –í–ù–£–¢–†–ò —Ç–µ–∫—Å—Ç–∞, –Ω–µ –≤ –∫–æ–Ω—Ü–µ
- –î–ª–∏–Ω–∞: 700-900 —Å–∏–º–≤–æ–ª–æ–≤ (—ç—Ç–æ –≤–∞–∂–Ω–æ –¥–ª—è Telegram caption!)

–°–¢–†–£–ö–¢–£–†–ê –ü–û–°–¢–ê:
```
[–≠–º–æ–¥–∑–∏] <b>–ó–∞–≥–æ–ª–æ–≤–æ–∫ ‚Äî –∫–æ—Ä–æ—Ç–∫–∏–π, —Ü–µ–ø–ª—è—é—â–∏–π</b>

–ü–µ—Ä–≤—ã–π –∞–±–∑–∞—Ü ‚Äî —Å—É—Ç—å –Ω–æ–≤–æ—Å—Ç–∏. –û —á—ë–º —Ä–µ—á—å, –ø–æ—á–µ–º—É —ç—Ç–æ –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ. 2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è.

–í—Ç–æ—Ä–æ–π –∞–±–∑–∞—Ü ‚Äî –¥–µ—Ç–∞–ª–∏. –ß—Ç–æ –∏–º–µ–Ω–Ω–æ –∏–∑–º–µ–Ω–∏–ª–æ—Å—å, –∫–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç, –∫–∞–∫–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏. –ó–¥–µ—Å—å –º–æ–∂–Ω–æ <a href="URL">–≤—Å—Ç–∞–≤–∏—Ç—å —Å—Å—ã–ª–∫—É</a> –Ω–∞ –∏—Å—Ç–æ—á–Ω–∏–∫ –∏–ª–∏ —Å–∞–º —Å–µ—Ä–≤–∏—Å.

–¢—Ä–µ—Ç–∏–π –∞–±–∑–∞—Ü ‚Äî –∫–æ–Ω—Ç–µ–∫—Å—Ç. –ü–æ—á–µ–º—É —ç—Ç–æ –≤–∞–∂–Ω–æ, –∫–∞–∫ —ç—Ç–æ –≤–ª–∏—è–µ—Ç –Ω–∞ —Ä—ã–Ω–æ–∫/–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π, —á—Ç–æ –¥—É–º–∞—é—Ç —ç–∫—Å–ø–µ—Ä—Ç—ã.

[–û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ] –ß–µ—Ç–≤—ë—Ä—Ç—ã–π –∞–±–∑–∞—Ü ‚Äî –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–≤–æ–¥ –∏–ª–∏ –≤–æ–ø—Ä–æ—Å –∫ —á–∏—Ç–∞—Ç–µ–ª—è–º.
```

–ê–ù–¢–ò-–ü–ê–¢–¢–ï–†–ù–´ (–ù–ò–ö–û–ì–î–ê –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–π):
- –®–∞–±–ª–æ–Ω–Ω—ã–µ CTA —Ç–∏–ø–∞ "üëâ –ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å", "üîó –°—Å—ã–ª–∫–∞"
- –ì–æ–ª—ã–µ URL –±–µ–∑ <a> —Ç–µ–≥–æ–≤
- "–ù–µ–π—Ä–æ—Å–µ—Ç—å" ‚Üí –∏—Å–ø–æ–ª—å–∑—É–π "AI" –∏–ª–∏ –Ω–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
- "–†–µ–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—ã–π", "—É–Ω–∏–∫–∞–ª—å–Ω—ã–π", "–ø—Ä–æ—Ä—ã–≤–Ω–æ–π"
- –ù–∞—á–∞–ª–æ —Å "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ–º...", "–í—Å—Ç—Ä–µ—á–∞–π—Ç–µ..."
- –°–ø–∏—Å–æ–∫ —Ñ–∏—á —á–µ—Ä–µ–∑ –±—É–ª–ª–µ—Ç—ã –≤ –∫–∞–∂–¥–æ–º –ø–æ—Å—Ç–µ
- –ü—É—Å—Ç—ã–µ –æ–±–µ—â–∞–Ω–∏—è –±–µ–∑ –∫–æ–Ω–∫—Ä–µ—Ç–∏–∫–∏

–ü–†–ê–í–ò–õ–ê –°–°–´–õ–û–ö:
- –°—Å—ã–ª–∫–∏ –≤—Å—Ç—Ä–∞–∏–≤–∞–π –í –¢–ï–ö–°–¢: <a href="URL">—á–∏—Ç–∞–π—Ç–µ –ø–æ–¥—Ä–æ–±–Ω–µ–µ</a>
- –¢–µ–∫—Å—Ç —Å—Å—ã–ª–∫–∏ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–º: "–ø–∏—à–µ—Ç TechCrunch", "—Å–æ–æ–±—â–∞–µ—Ç –∫–æ–º–ø–∞–Ω–∏—è"
- –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å 1-2 —Å—Å—ã–ª–∫–∏, –Ω–µ –±–æ–ª—å—à–µ
- –û—Å–Ω–æ–≤–Ω–∞—è —Å—Å—ã–ª–∫–∞ –Ω–∞ –∏—Å—Ç–æ—á–Ω–∏–∫: {article_link}

–°–¢–ê–¢–¨–Ø –î–õ–Ø –û–ë–†–ê–ë–û–¢–ö–ò:
–ó–∞–≥–æ–ª–æ–≤–æ–∫: {article.get('title', '')}
–ò—Å—Ç–æ—á–Ω–∏–∫: {source_name}
–û–ø–∏—Å–∞–Ω–∏–µ: {article.get('summary', '')[:800]}
–°—Å—ã–ª–∫–∞: {article_link}

–û—Ç–≤–µ—Ç –¢–û–õ–¨–ö–û JSON –±–µ–∑ markdown –±–ª–æ–∫–æ–≤:
{{"text": "–≥–æ—Ç–æ–≤—ã–π –ø–æ—Å—Ç —Å HTML-—Ä–∞–∑–º–µ—Ç–∫–æ–π, 700-900 —Å–∏–º–≤–æ–ª–æ–≤", "image_prompt": "DALL-E prompt in English, tech illustration style, modern, clean, 40 words max"}}"""


    def generate_post_for_rubric(
        self, article: Dict, rubric_name: str
    ) -> Optional[GeneratedPost]:
        """
        Generate a post for a specific rubric using its template.

        Args:
            article: Article data
            rubric_name: Name of the rubric (e.g., 'tool_review', 'news')

        Returns:
            GeneratedPost or None
        """
        try:
            from rubrics import Rubric, RUBRIC_PROMPTS

            # Get rubric enum
            try:
                rubric = Rubric(rubric_name)
            except ValueError:
                logger.warning(f"Unknown rubric: {rubric_name}, using default")
                return self.generate_post(article)

            # Get rubric-specific prompt template
            rubric_template = RUBRIC_PROMPTS.get(rubric)
            if not rubric_template:
                logger.warning(f"No template for rubric: {rubric_name}, using default")
                return self.generate_post(article)

            # Build full prompt
            article_link = article.get('link', '')
            source_name = article.get('source', '–∏—Å—Ç–æ—á–Ω–∏–∫')

            prompt = f"""–¢—ã ‚Äî –∫–æ–ø–∏—Ä–∞–π—Ç–µ—Ä Telegram-–∫–∞–Ω–∞–ª–∞ "AI –¥–ª—è –¥–æ–º–∞" (@ai_dlya_doma).

–ê–£–î–ò–¢–û–†–ò–Ø: –í—Å–µ, –∫—Ç–æ –∏–Ω—Ç–µ—Ä–µ—Å—É–µ—Ç—Å—è AI ‚Äî –æ—Ç –Ω–æ–≤–∏—á–∫–æ–≤ –¥–æ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö.

–†–£–ë–†–ò–ö–ê –ò –§–û–†–ú–ê–¢:
{rubric_template}

–°–¢–ê–¢–¨–Ø –î–õ–Ø –û–ë–†–ê–ë–û–¢–ö–ò:
–ó–∞–≥–æ–ª–æ–≤–æ–∫: {article.get('title', '')}
–ò—Å—Ç–æ—á–Ω–∏–∫: {source_name}
–û–ø–∏—Å–∞–Ω–∏–µ: {article.get('summary', '')[:800]}
–°—Å—ã–ª–∫–∞: {article_link}

–ü–†–ê–í–ò–õ–ê:
- –ò—Å–ø–æ–ª—å–∑—É–π HTML-—Ä–∞–∑–º–µ—Ç–∫—É Telegram: <b>, <i>, <a href="">
- –°—Å—ã–ª–∫–∏ –≤—Å—Ç—Ä–∞–∏–≤–∞–π –≤ —Ç–µ–∫—Å—Ç
- –î–ª–∏–Ω–∞: 700-900 —Å–∏–º–≤–æ–ª–æ–≤
- –ó–∞–º–µ–Ω–∏ [URL] –Ω–∞ —Ä–µ–∞–ª—å–Ω—É—é —Å—Å—ã–ª–∫—É: {article_link}

–û—Ç–≤–µ—Ç –¢–û–õ–¨–ö–û JSON:
{{"text": "–≥–æ—Ç–æ–≤—ã–π –ø–æ—Å—Ç", "image_prompt": "DALL-E prompt, 40 words"}}"""

            response = self._call_api(self.sonnet_model, prompt, max_tokens=1500)

            # Parse response
            try:
                cleaned = response.strip()
                cleaned = re.sub(r"^```json\s*", "", cleaned, flags=re.IGNORECASE)
                cleaned = re.sub(r"^```\s*", "", cleaned)
                cleaned = re.sub(r"\s*```$", "", cleaned)

                data = json.loads(cleaned)
                text = data.get("text", response)
                image_prompt = data.get("image_prompt")
            except (json.JSONDecodeError, TypeError, ValueError):
                text = response
                image_prompt = None

            text = validate_telegram_html(text)

            # Map rubric to PostFormat
            format_map = {
                "tool_review": PostFormat.AI_TOOL,
                "news": PostFormat.AI_TOOL,
                "prompt_home": PostFormat.PROMPT_DAY,
                "lifehack": PostFormat.QUICK_TIP,
                "free_service": PostFormat.AI_TOOL,
                "collection": PostFormat.CHECKLIST,
                "digest": PostFormat.CHECKLIST,
            }
            post_format = format_map.get(rubric_name, PostFormat.AI_TOOL)

            return GeneratedPost(
                text=text,
                format=post_format,
                article_url=article.get("link", ""),
                article_title=article.get("title", ""),
                image_prompt=image_prompt,
                image_url=article.get("image_url"),
            )

        except Exception as e:
            logger.error(f"Error generating post for rubric {rubric_name}: {e}")
            return None

    def generate_image_prompt(self, post: GeneratedPost) -> str:
        """
        Generate DALL-E prompt for post image.
        Uses Haiku for cost efficiency.
        """
        if post.image_prompt:
            return post.image_prompt

        prompt = f"""Create a DALL-E 3 image prompt for this Telegram post:

POST:
{post.text[:300]}

STYLE REQUIREMENTS:
- Flat design with soft gradients
- Pastel colors: light blue, pink, mint, lavender
- Minimalist icons
- Isometric perspective
- NO text on image
- NO people faces
- Cozy, friendly feeling
- Modern, clean look

Format: 1024x1024, English, 50-80 words.

Respond with ONLY the prompt, no explanations."""

        try:
            return self._call_api(self.haiku_model, prompt, max_tokens=150)
        except Exception as e:
            logger.error(f"Error generating image prompt: {e}")
            return "Flat design illustration, pastel colors, minimalist icons, cozy modern aesthetic, soft gradients, no text"

    def filter_and_rank_articles(
        self, articles: List[Dict], max_posts: int = 5
    ) -> List[tuple]:
        """
        Filter relevant articles and rank by confidence.

        Returns:
            List of (article, classification) tuples, sorted by confidence
        """
        classified = []

        for article in articles:
            result = self.classify_article(article)
            if result and result.get("relevant") and result.get("confidence", 0) >= 45:
                classified.append((article, result))
                logger.info(
                    f"Relevant: {article.get('title', '')[:50]}... "
                    f"(confidence: {result.get('confidence')})"
                )

        # Sort by confidence, take top N
        classified.sort(key=lambda x: x[1].get("confidence", 0), reverse=True)
        return classified[:max_posts]

    def generate_daily_posts(
        self, articles: List[Dict], count: int = 5
    ) -> List[GeneratedPost]:
        """
        Generate posts for the day from articles.

        Args:
            articles: List of news articles
            count: Number of posts to generate

        Returns:
            List of GeneratedPost objects
        """
        logger.info(f"Generating {count} posts from {len(articles)} articles")

        # Filter and rank articles
        ranked = self.filter_and_rank_articles(articles, max_posts=count)

        if not ranked:
            logger.warning("No relevant articles found")
            return []

        posts = []
        for article, classification in ranked:
            format_str = classification.get("format", "ai_tool")
            try:
                post_format = PostFormat(format_str)
            except ValueError:
                post_format = PostFormat.AI_TOOL

            post = self.generate_post(article, post_format)
            if post:
                # Generate image prompt if not present
                if not post.image_prompt:
                    post.image_prompt = self.generate_image_prompt(post)
                posts.append(post)
                logger.info(f"Generated post: {post.format.value}")

        return posts


if __name__ == "__main__":
    from dotenv import load_dotenv
    load_dotenv()

    generator = PostGenerator()

    # Test with dummy article
    test_article = {
        "title": "Canva launches AI photo editor for Instagram",
        "source": "TechCrunch",
        "summary": "Canva announced a new AI-powered photo editor that can automatically enhance photos, remove backgrounds, and suggest Instagram-ready filters. The tool is free for basic use.",
        "link": "https://example.com/canva-ai",
    }

    print("Testing classification...")
    result = generator.classify_article(test_article)
    print(f"Classification: {result}")

    if result and result.get("relevant"):
        print("\nGenerating post...")
        post = generator.generate_post(test_article, PostFormat.AI_TOOL)
        if post:
            print(f"\n{post.text}")
            print(f"\nImage prompt: {post.image_prompt}")
